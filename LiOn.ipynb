{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLX5gds9Y-Zh"
      },
      "source": [
        "# LiOn\n",
        "Lion 은 다양한 데이터와 연결되어 자연어 처리 분야에서의 전문성을 확장할 수 있는 모델입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yPPFPMZZzfn"
      },
      "source": [
        "## LiOnConnect\n",
        "Lion 모델의 강력한 자연어 처리 능력과 함께, 다양한 DB와의 연결을 통해 자연어 처리 분야에서 높은 성능과 효율성을 제공합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keoGAnQWY-W6"
      },
      "source": [
        "### How\n",
        "- [alpaca-lora.ipynb](https://colab.research.google.com/drive/1eWAmesrW99p7e1nah5bipn0zikMb8XYC#scrollTo=upOB2AQJSW9-): This notebook contains minimal code for **running [Alpaca-LoRA](https://github.com/tloen/alpaca-lora/)** for demonstration purposes.\n",
        "- [Make ChatGPT-replica](https://colab.research.google.com/drive/1UcLLV4mLtn8vxGk5U3TxiLNbVBealy16?usp=sharing):  ChatGPT를 만든 원리인 **GPT fine-tuning, 강화학습(PPO), RLHF, ChatGPT 데이터셋 구축**에 대해 다루고 코드 실습.\n",
        "- [LangChain](https://langchain.readthedocs.io/en/latest/index.html): 이 라이브러리로 **자연어로 데이터베이스 질의**.\n",
        "- [r-build/sf-restaurants-sql](https://github.com/r-build/sf-restaurants-sql): 샌프란시스코 식품 건강 조사 **데이터**.\n",
        "- [OpenAI GPT-3 and LangChain](https://blog.devgenius.io/query-database-using-natural-language-openai-gpt-3-d2403636527a): OpenAI GPT-3 및 LangChain을 사용하여 **자연어로 데이터베이스 질의**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIJNBCheY-Ub"
      },
      "source": [
        "### Related issues\n",
        "1. [비상업적 연구 목적으로만 본 소프트웨어의 파생 저작물을 제작할 수 있습니다.](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ExAe1UCWn5E"
      },
      "source": [
        "# Talk to Alpaca-LoRA\n",
        "\n",
        "This notebook contains minimal code for running [Alpaca-LoRA](https://github.com/tloen/alpaca-lora/) for demonstration purposes. Please check the repo for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT_l4GDTZ3Xa"
      },
      "source": [
        "## Use Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_pz8MuY84Qh",
        "outputId": "950bc913-6c4c-487b-bb12-24ade8e0b569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.9/dist-packages (0.37.1)\n",
            "\u001b[33m  WARNING: Did not find branch or tag 'c3dc391', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434,
          "referenced_widgets": [
            "2e962c1415d846eab7984e4343f81393",
            "ced7bf2faa8146f1b80618f11abdc1bc",
            "033cc572c99a44cba0d52fcd42316785",
            "5bb9376228e2422b8019ef0b5975c2d9",
            "e2cc3c89d3eb46d2b7ffb13d719a0c0f",
            "4e72f3dcd53844a6aa8fb84d7a05e819",
            "22bcdcff62644c3eb3f467a1018e7b7a",
            "a2a8650d5d2b4c30bc53ac48b7db319f",
            "f29fc3ad28b24d1d95a557409ad88827",
            "358e3b9e86b644ba830a3bc5ab1cad76",
            "81d3aa018f6b4a38acc76f7a76e9babf"
          ]
        },
        "id": "VucO3HSMoJkz",
        "outputId": "cce24758-a1b3-4215-d373-5a45c3fef9a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-2jfusqqjzzg6d --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e962c1415d846eab7984e4343f81393"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "\n",
        "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w3_lzwcqermJ"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Egh3beCVRpW5"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def evaluate(instruction, input=None):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        print(\"Response:\", output.split(\"### Response:\")[1].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPjzqKZIaHwk"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "upOB2AQJSW9-"
      },
      "outputs": [],
      "source": [
        "# while 1:\n",
        "#     evaluate(input(\"Instruction: \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RHtLmQcaJDu"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bIIxuOuYdI-5"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# model.save_pretrained('drive/MyDrive/LiOn/alpaca-lora-7b.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyx_2DXLeGzy"
      },
      "source": [
        "# MVP\n",
        "기업 데이터베이스(DB) 접속. **내부 정보를 실시간 접근**하여, 필요작업 수행.\n",
        "- 예: 기업 전화상담실\n",
        "- 내부 회계사와 변호사\n",
        "- 개인 비서와 의사\n",
        "- 글로벌 심리 상담사와 교수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHojNnMRZfQz"
      },
      "source": [
        "## Default Preferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KcsBhDGjeGH7"
      },
      "outputs": [],
      "source": [
        "# %cd drive/MyDrive/LiOn/\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMgS4iB-aStB"
      },
      "source": [
        "### Installation of required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84y1FjL7eQMF",
        "outputId": "97a290f7-8890-4ae7-b067-5fdd8951b9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.2)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (0.0.116)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.6)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.46)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymssql in /usr/local/lib/python3.9/dist-packages (2.2.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install langchain \n",
        "!pip install pymssql "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpSppJE2aWOL"
      },
      "source": [
        "### Setting things need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "APaXGjECr6mD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4130d1f8-bb3a-485c-bf95-394654355849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-4QK7Rblsp2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78620b4-8ce6-4b55-881b-2688e2734bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "olist_customers_dataset.csv\t  olist_products_dataset.csv\n",
            "olist_geolocation_dataset.csv\t  olist_sellers_dataset.csv\n",
            "olist_order_items_dataset.csv\t  olist_sellers_dataset.csv.db\n",
            "olist_order_payments_dataset.csv  product_category_name_translation.csv\n",
            "olist_order_reviews_dataset.csv   Untitled.ipynb\n",
            "olist_orders_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/LiOn/archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XZUmpjAs_WD",
        "outputId": "eb689e50-5ba3-4ab4-dbc7-22621fd3292a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "sqlite3 is already the newest version (3.31.1-4ubuntu0.5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AA7S7pb-eq42"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain import OpenAI, SQLDatabase, SQLDatabaseChain, PromptTemplate\n",
        "\n",
        "# https://drive.google.com/drive/folders/1YL3AD4oXy3mGTCB6RiXNCac8O7AYiPec?usp=sharing\n",
        "db = SQLDatabase.from_uri(f\"sqlite:////content/drive/MyDrive/LiOn/db/exported_data.db\")\n",
        "\n",
        "\n",
        "# # 데이터베이스 연결 정보\n",
        "# connection_string = \"postgres://username:password@hostname:port/database\"\n",
        "\n",
        "# # SQLDatabase 객체 생성\n",
        "# db = SQLDatabase.from_uri(connection_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "P7VSjJkCfY8o"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def chat(instruction, input=None):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        print(\"Response:\", output.split(\"### Response:\")[1].strip())\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "\n",
        "_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct MSSQL query to run, then look at the results of the query and return the answer.\n",
        "Descriptions of columns are shown below.\n",
        "--\n",
        "Name: 상품명\n",
        "ModelName: 모델명\n",
        "MakerName: 제조사명\n",
        "Price2: 공급가격\n",
        "--\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: \"Question here\"\n",
        "SQLQuery: \"SQL Query to run\"\n",
        "SQLResult: \"Result of the SQLQuery\"\n",
        "Answer: \"Final answer here\"\n",
        "\n",
        "Only use the following tables:\n",
        "\n",
        "\"Items\"\n",
        "\n",
        "Question: {input}\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(input_variables=[\"input\"], template=_DEFAULT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtKHD8-Omszi"
      },
      "source": [
        "## Running LangChain\n",
        "CustomSQLDatabaseChain, CustomOpenAI 를 만들어서 기존에 동작하지 않던 Model 사용 가능."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3YwLon_ftkdx"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SQLDatabaseChain\n",
        "\n",
        "class CustomSQLDatabaseChain(SQLDatabaseChain):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        llm = kwargs.pop(\"llm\", None)\n",
        "        super().__init__(llm=llm, *args, **kwargs)\n",
        "\n",
        "    def get_response(self, text):\n",
        "        prompt = self.prompt.format(input=text)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].cuda()\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=256\n",
        "        )\n",
        "        for s in generation_output.sequences:\n",
        "            output = tokenizer.decode(s)\n",
        "            response = output.split(\"### Response:\")[1].strip()\n",
        "            return {\"text\": response}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IoqH9uIjvooy"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI\n",
        "from typing import Any\n",
        "from pydantic import Field\n",
        "\n",
        "class CustomOpenAI(OpenAI):\n",
        "    model: Any = Field(default=None, description=\"The model to use for generating responses\")\n",
        "    tokenizer: Any = Field(default=None, description=\"The tokenizer to use with the model\")\n",
        "\n",
        "    def __call__(self, text, *args, **kwargs):\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].cuda()\n",
        "        generation_output = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=256\n",
        "        )\n",
        "        for s in generation_output.sequences:\n",
        "            output = self.tokenizer.decode(s)\n",
        "            response = output.split(\"### Response:\")[1].strip()\n",
        "            return {\"text\": response}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Up3n5nFv7adZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "ea9ef63c-38d7-459b-8293-9ad0fa154903"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a8cd26749dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m db_chain = CustomSQLDatabaseChain(\n\u001b[1;32m      5\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pydantic/main.cpython-39-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for CustomOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ],
      "source": [
        "### Prompts\n",
        "llm = CustomOpenAI(model=model, tokenizer=tokenizer)\n",
        "\n",
        "db_chain = CustomSQLDatabaseChain(\n",
        "    llm=llm,\n",
        "    database=db, prompt=PROMPT, verbose=True, return_intermediate_steps=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0yZnULY6vkp"
      },
      "outputs": [],
      "source": [
        "### Prompts\n",
        "llm = CustomOpenAI(model=model, tokenizer=tokenizer)\n",
        "\n",
        "db_chain = CustomSQLDatabaseChain(\n",
        "    llm=llm,\n",
        "    database=db, prompt=PROMPT, verbose=True, return_intermediate_steps=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeGVUFHr3Oyt",
        "outputId": "e876e1aa-0f44-4436-81b9-be284d3dd2c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LLaMAForCausalLM(\n",
              "      (model): LLaMAModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): RMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FecJxvpkanBC"
      },
      "outputs": [],
      "source": [
        "# ### Prompts\n",
        "# llm = CustomOpenAI(model=model, tokenizer=tokenizer)\n",
        "\n",
        "# db_chain = CustomSQLDatabaseChain(\n",
        "#     llm=llm,\n",
        "#     database=db, prompt=PROMPT, verbose=True, return_intermediate_steps=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORmRUHiJap7P"
      },
      "source": [
        "### Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3iwMz2JoZsj"
      },
      "outputs": [],
      "source": [
        "text1 = \"What are the top 3 most expensive products\"\n",
        "text2 = \"Can you give me a list of the 3 priciest items we have for sale?\"\n",
        "text3 = \"Which 3 products are the most expensive ones we offer?\"\n",
        "\n",
        "result = db_chain(text1)\n",
        "result = db_chain(text2)\n",
        "result = db_chain(text3)\n",
        "\n",
        "# for _ in range(15):\n",
        "#     text = input()\n",
        "#     result = db_chain(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D44uaf_qaweY"
      },
      "source": [
        "### ChatGPT API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9PSR7RWtynq"
      },
      "outputs": [],
      "source": [
        "# llm = CustomOpenAI(temperature=0)\n",
        "# db_chain = CustomSQLDatabaseChain(\n",
        "#     llm=llm,\n",
        "#     database=db, prompt=PROMPT, verbose=True, return_intermediate_steps=True\n",
        "# )\n",
        "# result = db_chain(\"가장 비싼 제품 3개를 알려줘\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e962c1415d846eab7984e4343f81393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ced7bf2faa8146f1b80618f11abdc1bc",
              "IPY_MODEL_033cc572c99a44cba0d52fcd42316785",
              "IPY_MODEL_5bb9376228e2422b8019ef0b5975c2d9"
            ],
            "layout": "IPY_MODEL_e2cc3c89d3eb46d2b7ffb13d719a0c0f"
          }
        },
        "ced7bf2faa8146f1b80618f11abdc1bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e72f3dcd53844a6aa8fb84d7a05e819",
            "placeholder": "​",
            "style": "IPY_MODEL_22bcdcff62644c3eb3f467a1018e7b7a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "033cc572c99a44cba0d52fcd42316785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2a8650d5d2b4c30bc53ac48b7db319f",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f29fc3ad28b24d1d95a557409ad88827",
            "value": 33
          }
        },
        "5bb9376228e2422b8019ef0b5975c2d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_358e3b9e86b644ba830a3bc5ab1cad76",
            "placeholder": "​",
            "style": "IPY_MODEL_81d3aa018f6b4a38acc76f7a76e9babf",
            "value": " 33/33 [00:17&lt;00:00,  1.87it/s]"
          }
        },
        "e2cc3c89d3eb46d2b7ffb13d719a0c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e72f3dcd53844a6aa8fb84d7a05e819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22bcdcff62644c3eb3f467a1018e7b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2a8650d5d2b4c30bc53ac48b7db319f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f29fc3ad28b24d1d95a557409ad88827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "358e3b9e86b644ba830a3bc5ab1cad76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81d3aa018f6b4a38acc76f7a76e9babf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}